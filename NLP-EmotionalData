import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout


train_df = pd.read_csv("/kaggle/input/emotions-dataset-for-nlp/train.txt", sep=";", names=["text", "emotion"])
test_df  = pd.read_csv("/kaggle/input/emotions-dataset-for-nlp/test.txt", sep=";", names=["text", "emotion"])
val_df   = pd.read_csv("/kaggle/input/emotions-dataset-for-nlp/val.txt", sep=";", names=["text", "emotion"])

# Combine train + val for full training set
df = pd.concat([train_df, val_df], ignore_index=True)

print(df.head())

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize


#  Distribution of emotions
plt.figure(figsize=(8,5))
sns.countplot(x="emotion", data=df, order=df['emotion'].value_counts().index, palette="Set2")
plt.title("Distribution of Emotions", fontsize=14)
plt.xticks(rotation=45)
plt.show()

#  Sentence length analysis
df["sentence_length"] = df["text"].apply(lambda x: len(word_tokenize(str(x))))
plt.figure(figsize=(8,5))
sns.histplot(df["sentence_length"], bins=30, kde=True, color="purple")
plt.title("Sentence Length Distribution", fontsize=14)
plt.xlabel("Number of words")
plt.show()

#  WordCloud for all text
all_text = " ".join(df["text"].astype(str))
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(all_text)

plt.figure(figsize=(12,6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("WordCloud of All Texts", fontsize=16)
plt.show()

#  WordCloud per emotion
emotions = df["emotion"].unique()
for emotion in emotions:
    text = " ".join(df[df["emotion"]==emotion]["text"].astype(str))
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
    plt.figure(figsize=(10,5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"WordCloud for {emotion}", fontsize=14)
    plt.show()

texts = df['text'].astype(str).tolist()
labels = df['emotion'].tolist()

encoder = LabelEncoder()
y = encoder.fit_transform(labels)
max_words = 10000   # keep the top 10k most frequent words
max_len = 100       # maximum length of each text

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, maxlen=max_len, padding='post')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Sequential([
    Embedding(input_dim=max_words, output_dim=100, input_length=max_len),
    Conv1D(128, 5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(encoder.classes_), activation='softmax')  # number of classes
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(X_train, y_train,
                    epochs=5,
                    batch_size=32,
                    validation_split=0.2)
loss, acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {acc:.2f}")

sample_texts = [
    "I am so happy today!",
    "I feel very sad and lonely",
    "This makes me really angry!",
    "I am surprised by the news"
]

seq = tokenizer.texts_to_sequences(sample_texts)
padded = pad_sequences(seq, maxlen=max_len)
pred = model.predict(padded)

for i, t in enumerate(sample_texts):
    emotion = encoder.classes_[pred[i].argmax()]
    print(f"{t} â†’ {emotion}")
